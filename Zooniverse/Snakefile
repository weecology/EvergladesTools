import glob,os
import datetime
import os
from hashlib import md5
from snakemake.io import glob_wildcards, expand

import pandas as pd
import numpy as np
import pandas as pd
import os

from .nest_detection import detect_nests

input_file = "/blue/ewhite/everglades/orthomosaics/detail.csv"
dir_path = "/blue/ewhite/everglades/orthomosaics/"
CURRENT_FILES = []


def getmd5(file_path=None, encoding='utf-8'):
    """Get MD5 of a data source."""
    checksum = md5()
    input_file = open(file_path, 'r', encoding=encoding)
    for line in input_file:
        checksum.update(str(line).encode(encoding))
    return checksum.hexdigest()


def current_files():
    current_fls = []
    for root, dirs, files in os.walk(dir_path, topdown=False):
        for name in files:
            root = os.path.abspath(root)
            print(name)
            if name.strip().endswith(".shp"):
                x = os.path.normpath(os.path.join(root, name))
                current_fls.append(x)
    CURRENT_FILES = current_fls
    return current_fls


def update_md5(cur_files=None, details_file="detail.csv"):
    cur_files = current_files()  # get the current updated files after run
    details = details_file
    with open(details, 'w') as dp:
        for fp in cur_files:
            md5_sum = getmd5(fp)
            dp.write("{fp},{md5_sum}\n".format(fp=fp, md5_sum=md5_sum))


def compare_md5(fpath, last_md5):
    cur_md5 = getmd5(file_path=fpath)
    return cur_md5 == last_md5


def aggregate_input(wildcards):
    # def aggregate_input(input_file, dir_path="."):
    for root, dirs, files in os.walk(dir_path, topdown=False):
        for name in files:
            root = os.path.abspath(root)
            if name.strip().endswith(".shp"):
                x = os.path.normpath(os.path.join(root, name))
                # print(x)
                CURRENT_FILES.append(x)
    path_md = []
    with open(input_file) as f:
        path_md = f.readlines()
    path_md = [r.split(",") for r in path_md]
    path_md = [(r[0].strip(), r[1].strip()) for r in path_md]
    OLD_FILES = [r[0] for r in path_md]
    modified_files = [r[0] for r in path_md if not compare_md5(r[0], r[1])]
    changed_files = (set(CURRENT_FILES) - set(OLD_FILES))
    return list(changed_files.union(set(modified_files)))


def get_processed_nests_output(changed_files, year_index, site_index):
    year_site = set()
    for i in changed_files:
        year_site_index = i.strip().strip("/").split(os.path.sep)
        year, site = year_site_index[year_index], year_site_index[site_index]
        year_site_dir = str(year) + "_" + str(site)
        year_site.add(year_site_dir)
        if not os.path.exists("output/" + year_site_dir):
            os.makedirs("output/" + year_site_dir)
    return [f"output/{x}/processed.shp".format(x) for x in year_site]


def get_changed_dirs(changed_files, year_index, site_index):
    year_site = set()
    for i in changed_files:
        year_site_index = i.strip().strip("/").split(os.path.sep)
        x = year_site_index[year_index], year_site_index[site_index]
        year_site.add(x)
    return list(year_site)


rule all:
    input: "nest_detect_log.txt", "process_nests_log.txt", "combine_nests_log.txt", "update_details_log.txt"


rule nest_detect:
    input:
        one=aggregate_input
    output: "nest_detect_log.txt", "process_nests_log.txt", "combine_nests_log.txt"
	run:
	    year_index = 5
	    site_index = 6
	    dirs_Year_site = get_changed_dirs(input.one, year_index, site_index)
	    for y_s in dirs_Year_site:
	        path_arg = os.path.join(dir_path,y_s[0],y_s[1])
 	        detect_nests(path_arg, y_s[0], y_s[1])

	    os.system("echo 22 >> combine_nests_log.txt")
	    os.system("echo 22 >> nest_detect_log.txt")
	    os.system("echo 22 >> process_nests_log.txt")


rule update_details:
    input: "combine_nests_log.txt"
    output: "update_details_log.txt"
    run:
	    update_md5(current_files())
	    os.system("echo 22 >> update_details_log.txt")

